{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, LabelBinarizer, StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_file_path(track_id, tidigits_path='/home/cong/Downloads/fma_small'):\n",
    "    tidigits_path = tidigits_path + '/{:03d}/{:06d}.mp3'.format(track_id // 1000, track_id)\n",
    "    return tidigits_path\n",
    "\n",
    "def preprocess_audio_file(file_path, n_mfcc=13, max_pad_len=2048):\n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(file_path, mono=True, duration=30)\n",
    "    # Extract MFCCs\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    # Pad MFCCs\n",
    "    pad_width = max_pad_len - mfcc.shape[1]\n",
    "    mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "    return mfcc\n",
    "\n",
    "# Example usage\n",
    "# mfcc_features = preprocess_audio_file(\"/home/cong/Downloads/fma_small/000/000005.mp3\")\n",
    "# Note: You'll need to preprocess all audio files in your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((106574, 52), (106574, 518), (13129, 249))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "features = utils.load('data/fma_metadata/features.csv')\n",
    "echonest = utils.load('data/fma_metadata/echonest.csv')\n",
    "\n",
    "np.testing.assert_array_equal(features.index, tracks.index)\n",
    "assert echonest.index.isin(tracks.index).all()\n",
    "\n",
    "tracks.shape, features.shape, echonest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough Echonest features: (13129, 767)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((8000, 52), (8000, 518))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = tracks.index[tracks['set', 'subset'] <= 'small']\n",
    "\n",
    "assert subset.isin(tracks.index).all()\n",
    "assert subset.isin(features.index).all()\n",
    "\n",
    "features_all = features.join(echonest, how='inner').sort_index(axis=1)\n",
    "print('Not enough Echonest features: {}'.format(features_all.shape))\n",
    "\n",
    "tracks = tracks.loc[subset]\n",
    "features_all = features.loc[subset]\n",
    "\n",
    "tracks.shape, features_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400 training examples, 800 validation examples, 800 testing examples\n",
      "Top genres (8): ['Electronic', 'Experimental', 'Folk', 'Hip-Hop', 'Instrumental', 'International', 'Pop', 'Rock']\n",
      "All genres (114): [1, 2, 6, 10, 12, 15, 16, 17, 18, 21, 22, 25, 26, 27, 30, 31, 32, 33, 36, 38, 41, 42, 45, 46, 47, 49, 53, 58, 64, 66, 70, 71, 76, 77, 79, 81, 83, 85, 86, 88, 89, 90, 92, 94, 98, 100, 101, 102, 103, 107, 109, 111, 113, 117, 118, 125, 130, 167, 171, 172, 174, 177, 180, 181, 182, 183, 184, 185, 186, 214, 224, 232, 236, 240, 247, 250, 267, 286, 296, 297, 314, 337, 359, 360, 361, 362, 400, 401, 404, 439, 440, 456, 468, 491, 495, 502, 504, 514, 524, 538, 539, 542, 580, 602, 619, 695, 741, 763, 808, 811, 1032, 1060, 1193, 1235]\n"
     ]
    }
   ],
   "source": [
    "train = tracks.index[tracks['set', 'split'] == 'training']\n",
    "val = tracks.index[tracks['set', 'split'] == 'validation']\n",
    "test = tracks.index[tracks['set', 'split'] == 'test']\n",
    "\n",
    "print('{} training examples, {} validation examples, {} testing examples'.format(*map(len, [train, val, test])))\n",
    "\n",
    "genres = list(LabelEncoder().fit(tracks['track', 'genre_top']).classes_)\n",
    "#genres = list(tracks['track', 'genre_top'].unique())\n",
    "print('Top genres ({}): {}'.format(len(genres), genres))\n",
    "genres = list(MultiLabelBinarizer().fit(tracks['track', 'genres_all']).classes_)\n",
    "print('All genres ({}): {}'.format(len(genres), genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>track_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154308</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154309</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154413</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154414</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155066</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  1  2  3  4  5  6  7\n",
       "track_id                        \n",
       "2         0  0  0  1  0  0  0  0\n",
       "5         0  0  0  1  0  0  0  0\n",
       "10        0  0  0  0  0  0  1  0\n",
       "140       0  0  1  0  0  0  0  0\n",
       "141       0  0  1  0  0  0  0  0\n",
       "...      .. .. .. .. .. .. .. ..\n",
       "154308    0  0  0  1  0  0  0  0\n",
       "154309    0  0  0  1  0  0  0  0\n",
       "154413    0  0  0  0  0  0  1  0\n",
       "154414    0  0  0  0  0  0  1  0\n",
       "155066    0  0  0  1  0  0  0  0\n",
       "\n",
       "[8000 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_onehot = LabelBinarizer().fit_transform(tracks['track', 'genre_top'])\n",
    "labels_onehot = pd.DataFrame(labels_onehot, index=tracks.index)\n",
    "labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_onehot.iloc[2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc_features(block):\n",
    "    result_mfcc = []\n",
    "    result_label = []\n",
    "    for idx, track_id in enumerate(block):\n",
    "        mfcc = None\n",
    "        label = None\n",
    "        try:\n",
    "            mfcc = preprocess_audio_file(get_file_path(track_id))\n",
    "            label = labels_onehot.iloc[idx].values.tolist()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if mfcc is not None:\n",
    "            result_mfcc.append(mfcc)\n",
    "            result_label.append(label)\n",
    "    \n",
    "    return np.array(result_mfcc), np.array(result_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mfcc, train_label = get_mfcc_features(train)\n",
    "val_mfcc, val_label = get_mfcc_features(val)\n",
    "test_mfcc, test_label = get_mfcc_features(test)\n",
    "\n",
    "# train_mfcc = np.load('data/train_mfcc.npy')\n",
    "# train_label = np.load('data/train_label.npy')\n",
    "# val_mfcc = np.load('data/val_mfcc.npy')\n",
    "# val_label = np.load('data/val_label.npy')\n",
    "# test_mfcc = np.load('data/test_mfcc.npy')\n",
    "# test_label = np.load('data/test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('data/train_mfcc.npy', train_mfcc)\n",
    "# np.save('data/train_label.npy', train_label)\n",
    "# np.save('data/val_mfcc.npy', val_mfcc)\n",
    "# np.save('data/val_label.npy', val_label)\n",
    "# np.save('data/test_mfcc.npy', test_mfcc)\n",
    "# np.save('data/test_label.npy', test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6394, 13, 2048),\n",
       " (6394, 8),\n",
       " (800, 13, 2048),\n",
       " (800, 8),\n",
       " (800, 13, 2048),\n",
       " (800, 8))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mfcc.shape, train_label.shape, val_mfcc.shape, val_label.shape, test_mfcc.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MusicGenreClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MusicGenreClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 8)  # Output layer with 8 neurons\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)  # No softmax here as BCEWithLogitsLoss will be used\n",
    "        return out\n",
    "\n",
    "# Example instantiation of the model\n",
    "model_MLP = MusicGenreClassifier(input_size=13*2048, hidden_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNMusicGenreClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNMusicGenreClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Calculate the size of the flattened features after conv and pooling layers\n",
    "        self.fc1 = nn.Linear(64 * 256, 500)  # Adjusted input size\n",
    "        self.fc2 = nn.Linear(500, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 256)  # Flatten and adjust size accordingly\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=25):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (mfccs, labels) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            mfccs = mfccs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(mfccs)\n",
    "            loss = criterion(outputs, labels.float())  # Ensure labels are float\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for mfccs, labels in test_loader:\n",
    "            mfccs = mfccs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(mfccs)\n",
    "            predicted = (outputs > 0.0).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item() / labels.size(1)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on test set: {accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, mfccs, labels):\n",
    "        \"\"\"\n",
    "        file_paths: List of paths to audio files\n",
    "        labels: Corresponding one-hot encoded labels\n",
    "        \"\"\"\n",
    "        self.mfccs = mfccs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc = self.mfccs[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return mfcc, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 10:40:11.709960: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-20 10:40:12.426679: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "model = CNNMusicGenreClassifier(num_classes=8)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mfcc_tensor = torch.from_numpy(train_mfcc).float()\n",
    "train_label_tensor = torch.from_numpy(train_label).float()\n",
    "test_mfcc_tensor = torch.from_numpy(test_mfcc).float()\n",
    "test_label_tensor = torch.from_numpy(test_label).float()\n",
    "\n",
    "# train_mfcc_tensor = train_mfcc_tensor.unsqueeze(1)\n",
    "# test_mfcc_tensor = test_mfcc_tensor.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instances\n",
    "train_dataset = AudioDataset(train_mfcc_tensor, train_label_tensor)\n",
    "test_dataset = AudioDataset(test_mfcc_tensor, test_label_tensor)\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Step [100/200], Loss: 0.3741\n",
      "Epoch [1/25], Step [200/200], Loss: 0.3785\n",
      "Epoch [2/25], Step [100/200], Loss: 0.3789\n",
      "Epoch [2/25], Step [200/200], Loss: 0.3741\n",
      "Epoch [3/25], Step [100/200], Loss: 0.3893\n",
      "Epoch [3/25], Step [200/200], Loss: 0.3733\n",
      "Epoch [4/25], Step [100/200], Loss: 0.3753\n",
      "Epoch [4/25], Step [200/200], Loss: 0.3845\n",
      "Epoch [5/25], Step [100/200], Loss: 0.3789\n",
      "Epoch [5/25], Step [200/200], Loss: 0.3753\n",
      "Epoch [6/25], Step [100/200], Loss: 0.3719\n",
      "Epoch [6/25], Step [200/200], Loss: 0.3797\n",
      "Epoch [7/25], Step [100/200], Loss: 0.3838\n",
      "Epoch [7/25], Step [200/200], Loss: 0.3744\n",
      "Epoch [8/25], Step [100/200], Loss: 0.3829\n",
      "Epoch [8/25], Step [200/200], Loss: 0.3795\n",
      "Epoch [9/25], Step [100/200], Loss: 0.3733\n",
      "Epoch [9/25], Step [200/200], Loss: 0.3844\n",
      "Epoch [10/25], Step [100/200], Loss: 0.3783\n",
      "Epoch [10/25], Step [200/200], Loss: 0.3813\n",
      "Epoch [11/25], Step [100/200], Loss: 0.3831\n",
      "Epoch [11/25], Step [200/200], Loss: 0.3762\n",
      "Epoch [12/25], Step [100/200], Loss: 0.3821\n",
      "Epoch [12/25], Step [200/200], Loss: 0.3772\n",
      "Epoch [13/25], Step [100/200], Loss: 0.3783\n",
      "Epoch [13/25], Step [200/200], Loss: 0.3851\n",
      "Epoch [14/25], Step [100/200], Loss: 0.3765\n",
      "Epoch [14/25], Step [200/200], Loss: 0.3665\n",
      "Epoch [15/25], Step [100/200], Loss: 0.3768\n",
      "Epoch [15/25], Step [200/200], Loss: 0.3764\n",
      "Epoch [16/25], Step [100/200], Loss: 0.3739\n",
      "Epoch [16/25], Step [200/200], Loss: 0.3790\n",
      "Epoch [17/25], Step [100/200], Loss: 0.3789\n",
      "Epoch [17/25], Step [200/200], Loss: 0.3788\n",
      "Epoch [18/25], Step [100/200], Loss: 0.3823\n",
      "Epoch [18/25], Step [200/200], Loss: 0.3769\n",
      "Epoch [19/25], Step [100/200], Loss: 0.3764\n",
      "Epoch [19/25], Step [200/200], Loss: 0.3723\n",
      "Epoch [20/25], Step [100/200], Loss: 0.3760\n",
      "Epoch [20/25], Step [200/200], Loss: 0.3787\n",
      "Epoch [21/25], Step [100/200], Loss: 0.3743\n",
      "Epoch [21/25], Step [200/200], Loss: 0.3786\n",
      "Epoch [22/25], Step [100/200], Loss: 0.3735\n",
      "Epoch [22/25], Step [200/200], Loss: 0.3788\n",
      "Epoch [23/25], Step [100/200], Loss: 0.3729\n",
      "Epoch [23/25], Step [200/200], Loss: 0.3729\n",
      "Epoch [24/25], Step [100/200], Loss: 0.3801\n",
      "Epoch [24/25], Step [200/200], Loss: 0.3814\n",
      "Epoch [25/25], Step [100/200], Loss: 0.3730\n",
      "Epoch [25/25], Step [200/200], Loss: 0.3836\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, criterion, optimizer, device, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 87.5%\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNMusicGenreClassifier(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=16384, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=8, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
